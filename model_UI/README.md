main.py

model_choice:
canting all the models to choose
when adding the new model, add the model name here and create a new function to call
new function reference to current Lamma 3 and stable code

current default model:
Meta Llama 3 8B: with port 1234
Stable Code Instruct 3b: with port 1235

p.s. gpt3.5 can also use the same way to connect with base_url and api_key (p.s. need credit on OpenAI account to work)

Running LM Studio API server:
open the local server and choose the correct model with the correct port
multiple models can be loaded with multiple LM Studio

resource/nad_dic.txt
canting all words using for detection on level_4
